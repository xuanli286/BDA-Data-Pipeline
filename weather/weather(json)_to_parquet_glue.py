{
	"jobConfig": {
		"name": "weather_to_parquet",
		"description": "",
		"role": "arn:aws:iam::324037293111:role/service-role/AWSGlueServiceRole-is459-project-airline-performance-crawler",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "weather_to_parquet.py",
		"scriptLocation": "s3://aws-glue-assets-324037293111-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-11-08T02:51:25.269Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-324037293111-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-324037293111-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.dynamicframe import DynamicFrame\r\nfrom pyspark.sql.functions import input_file_name, regexp_extract\r\nfrom pyspark.sql import DataFrame\r\n\r\n# Initialize Glue and Spark Contexts\r\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args[\"JOB_NAME\"], args)\r\n\r\n# Define source and target buckets\r\nsource_bucket = \"s3://is459-project-data/weather/weather_data/\"\r\ntarget_bucket = \"s3://is459-project-data/weather/weather_data_parquet/\"\r\n\r\n# Recursively load JSON files from each folder in the source bucket\r\ninputDF = spark.read.json(f\"{source_bucket}*/*.json\")\r\n\r\n# Extract year and month from the file path to maintain folder structure\r\ninputDF = inputDF.withColumn(\"year\", regexp_extract(input_file_name(), r\".*/(\\d{4})/.*\", 1))\r\ninputDF = inputDF.withColumn(\"month\", regexp_extract(input_file_name(), r\".*/(\\d{4})/(\\d{2})\\.json\", 2))\r\n\r\n# Convert to Glue DynamicFrame\r\ndynamic_frame = DynamicFrame.fromDF(inputDF, glueContext, \"dynamic_frame\")\r\n\r\n# Write each year/month partition as Parquet\r\nglueContext.write_dynamic_frame.from_options(\r\n    frame=dynamic_frame,\r\n    connection_type=\"s3\",\r\n    connection_options={\r\n        \"path\": target_bucket,\r\n        \"partitionKeys\": [\"year\", \"month\"]\r\n    },\r\n    format=\"parquet\"\r\n)\r\n\r\n# Commit the job\r\njob.commit()\r\n"
}