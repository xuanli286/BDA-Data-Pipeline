{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042bedf7-5669-4df7-90cb-fc12d4f0eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deep_translator\n",
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7095d00-d0fd-4fcd-b0e0-2a7f25a98522",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e814c-f869-4f66-b8de-e00605ebf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f66f9-9d1b-4700-ba3a-14223e24fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72133977-819e-4128-b948-c5f06b0aab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xuanli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/xuanli/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from abc import abstractmethod\n",
    "import collections\n",
    "from datetime import datetime\n",
    "from deep_translator import GoogleTranslator\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bbb6ee-d1f7-4ce7-bb72-d5e2399b1cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93523855-041b-46fa-be0c-2bc79b12fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_df = pd.read_json('../data/reddit_posts.json')\n",
    "# comments_df = pd.read_json('../data/reddit_comments.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e028d58-f4cd-423b-b7d4-35b37616514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from S3 bucket - temporary \n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def load_json(prefix):\n",
    "    response = s3.list_objects_v2(Bucket=\"is459-project-data\", Prefix=prefix)\n",
    "    json_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.json')]\n",
    "    combined_data = []\n",
    "\n",
    "    for file_key in json_files:\n",
    "        obj = s3.get_object(Bucket=\"is459-project-data\", Key=file_key)\n",
    "        data = json.load(BytesIO(obj['Body'].read()))\n",
    "        if isinstance(data, list):\n",
    "            combined_data.extend(data)\n",
    "        else:\n",
    "            combined_data.append(data)\n",
    "            \n",
    "    df = pd.DataFrame(combined_data)\n",
    "    return df\n",
    "\n",
    "posts_df = load_json(\"reddit/posts/\")\n",
    "comments_df = load_json(\"reddit/comments/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab406f5-cfc4-4d34-b6a9-3986440830ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5209, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ecd653-9bde-4f8d-a215-2076627b0d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122499, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef5ba38-ce19-4fab-8265-8b4a3947e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df.replace(\"\", np.nan)\n",
    "posts_df.dropna(inplace=True)\n",
    "\n",
    "comments_df = comments_df.replace(\"\", np.nan)\n",
    "comments_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c92ff0f-3516-4fd9-b60d-03c0caf4152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = {\n",
    "    'SouthwestAirlines': 'WN', \n",
    "    'Southwest_Airlines': 'WN', \n",
    "    'AmericanAir': 'AA',\n",
    "    'DeltaAirlines': 'DL',\n",
    "    'HawaiianAirlines': 'HA',\n",
    "    'frontierairlines': 'F9'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acac675c-2a46-4056-84ed-4e9d34c251b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df['Code'] = posts_df['subreddit'].map(airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e25dae-7b84-445e-b17c-ca41ab786733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>username</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tdjs75</td>\n",
       "      <td>2022-03-14 07:40:58</td>\n",
       "      <td>Accidentally volunteered for flight voucher</td>\n",
       "      <td>Can anyone help with their stories and experie...</td>\n",
       "      <td>anishabhakri</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>AmericanAir</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18js83r</td>\n",
       "      <td>2023-12-16 22:21:43</td>\n",
       "      <td>A321 Airplane row 7 and 8</td>\n",
       "      <td>Who have the right to open or close the shade?</td>\n",
       "      <td>The-peace-of-war</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AmericanAir</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17kcrdf</td>\n",
       "      <td>2023-10-31 12:18:06</td>\n",
       "      <td>Can’t upgrade American air flight</td>\n",
       "      <td>Has anyone ever had this happen? My husband an...</td>\n",
       "      <td>Beautiful-Solid-1750</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>AmericanAir</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1cfekj1</td>\n",
       "      <td>2024-04-29 03:21:29</td>\n",
       "      <td>How do I get credit for my miles flown?</td>\n",
       "      <td>I’ll try to be brief, I took a flight on Ameri...</td>\n",
       "      <td>ravenbythesea</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AmericanAir</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1afss9q</td>\n",
       "      <td>2024-02-01 05:09:41</td>\n",
       "      <td>Domestic first class on 737-800</td>\n",
       "      <td>I’m going to be flying domestic first class on...</td>\n",
       "      <td>diadw</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>AmericanAir</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>1fwpz6b</td>\n",
       "      <td>2024-10-05 21:06:15</td>\n",
       "      <td>10000 Miles Promo</td>\n",
       "      <td>\\nWill the current 10000 Miles promo be eligib...</td>\n",
       "      <td>MayorShinn</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5205</th>\n",
       "      <td>1fwjhgn</td>\n",
       "      <td>2024-10-05 13:40:18</td>\n",
       "      <td>1 hr 46 min layover in Denver -- Yes or No?</td>\n",
       "      <td>EDIT: thanks for the reassurance everyone! i b...</td>\n",
       "      <td>carrotsoup3</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>1fwesa2</td>\n",
       "      <td>2024-10-05 09:03:36</td>\n",
       "      <td>Frontier airlines app issues</td>\n",
       "      <td>I booked a flight &amp; am trying to log in, but t...</td>\n",
       "      <td>AnxietyDue61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>1fwelej</td>\n",
       "      <td>2024-10-05 08:53:51</td>\n",
       "      <td>No GWP flights</td>\n",
       "      <td>I live in Las Vegas and have the Go Wild Pass....</td>\n",
       "      <td>FantasticScratch5719</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>1fwe0df</td>\n",
       "      <td>2024-10-05 08:23:56</td>\n",
       "      <td>Frontier agent requested my full credit card n...</td>\n",
       "      <td>I was chatting with a frontier agent on the of...</td>\n",
       "      <td>WolverineHelpful9775</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4504 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                 date  \\\n",
       "0      tdjs75  2022-03-14 07:40:58   \n",
       "1     18js83r  2023-12-16 22:21:43   \n",
       "2     17kcrdf  2023-10-31 12:18:06   \n",
       "3     1cfekj1  2024-04-29 03:21:29   \n",
       "4     1afss9q  2024-02-01 05:09:41   \n",
       "...       ...                  ...   \n",
       "5204  1fwpz6b  2024-10-05 21:06:15   \n",
       "5205  1fwjhgn  2024-10-05 13:40:18   \n",
       "5206  1fwesa2  2024-10-05 09:03:36   \n",
       "5207  1fwelej  2024-10-05 08:53:51   \n",
       "5208  1fwe0df  2024-10-05 08:23:56   \n",
       "\n",
       "                                                  title  \\\n",
       "0           Accidentally volunteered for flight voucher   \n",
       "1                             A321 Airplane row 7 and 8   \n",
       "2                     Can’t upgrade American air flight   \n",
       "3               How do I get credit for my miles flown?   \n",
       "4                       Domestic first class on 737-800   \n",
       "...                                                 ...   \n",
       "5204                                  10000 Miles Promo   \n",
       "5205        1 hr 46 min layover in Denver -- Yes or No?   \n",
       "5206                       Frontier airlines app issues   \n",
       "5207                                     No GWP flights   \n",
       "5208  Frontier agent requested my full credit card n...   \n",
       "\n",
       "                                                content              username  \\\n",
       "0     Can anyone help with their stories and experie...          anishabhakri   \n",
       "1        Who have the right to open or close the shade?      The-peace-of-war   \n",
       "2     Has anyone ever had this happen? My husband an...  Beautiful-Solid-1750   \n",
       "3     I’ll try to be brief, I took a flight on Ameri...         ravenbythesea   \n",
       "4     I’m going to be flying domestic first class on...                 diadw   \n",
       "...                                                 ...                   ...   \n",
       "5204  \\nWill the current 10000 Miles promo be eligib...            MayorShinn   \n",
       "5205  EDIT: thanks for the reassurance everyone! i b...           carrotsoup3   \n",
       "5206  I booked a flight & am trying to log in, but t...          AnxietyDue61   \n",
       "5207  I live in Las Vegas and have the Go Wild Pass....  FantasticScratch5719   \n",
       "5208  I was chatting with a frontier agent on the of...  WolverineHelpful9775   \n",
       "\n",
       "      commentCount  score         subreddit Code  \n",
       "0                1      3       AmericanAir   AA  \n",
       "1                1      1       AmericanAir   AA  \n",
       "2                6      0       AmericanAir   AA  \n",
       "3                2      1       AmericanAir   AA  \n",
       "4                3      1       AmericanAir   AA  \n",
       "...            ...    ...               ...  ...  \n",
       "5204             2      1  frontierairlines   F9  \n",
       "5205            29      3  frontierairlines   F9  \n",
       "5206             1      1  frontierairlines   F9  \n",
       "5207             5      1  frontierairlines   F9  \n",
       "5208            19     13  frontierairlines   F9  \n",
       "\n",
       "[4504 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d36850-0f6f-4eb1-b748-dd3b7bc43dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df.drop_duplicates(subset=\"id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8cc77a-162b-492a-867f-9127c30b75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df.drop_duplicates(subset=\"id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b7d6877-6071-4209-8ab9-f0c2785796a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3496, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8aa442-2b6a-4e5f-899e-d2eef2ea432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118698, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2297862-f719-4c60-ad3c-732d498415a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_post_dict = posts_df.set_index('id')['Code'].to_dict()\n",
    "comments_df['Code'] = comments_df['post_id'].map(code_post_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19648196-2638-487a-b81d-3b40203aa10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xuanli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "DetectorFactory.seed = 42\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45af1fac-5f63-4296-8e2d-15746ecb6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = GoogleTranslator(source='auto', target='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba55e52e-863d-4d6c-b642-b6ba0fbdd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Check if text is in English\n",
    "\n",
    "    Args:\n",
    "    text (str): text to check\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def chunk_text(text, max_length=5000):\n",
    "    chunks = []\n",
    "    while len(text) > max_length:\n",
    "        split_index = text[:max_length].rfind(' ')\n",
    "        if split_index == -1:\n",
    "            split_index = max_length\n",
    "        chunks.append(text[:split_index])\n",
    "        text = text[split_index:].strip()\n",
    "    chunks.append(text)\n",
    "    return chunks\n",
    "\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        if not is_english(text):\n",
    "            if len(text) > 5000:\n",
    "                chunks = chunk_text(text)\n",
    "                translated_chunks = [translator.translate(chunk) for chunk in chunks]\n",
    "                return ' '.join(translated_chunks)\n",
    "            else:\n",
    "                return translator.translate(text)\n",
    "        else:\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf69b6ef-988e-4a7a-97e2-67cc049e8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by converting to lowercase, removing numbers, punctuation, and stopwords\n",
    "\n",
    "    Args:\n",
    "    text (str): text to preprocess\n",
    "\n",
    "    Returns:\n",
    "    text (str): preprocessed text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0db8888-7233-453c-b9f9-dfef92c2dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect(df, vectorizer=None, lda_model=None, topic_dict=None):\n",
    "    \"\"\"\n",
    "    Get aspect of text using LDA model.\n",
    "\n",
    "    Args:\n",
    "    text (str): text to extract aspect from\n",
    "    vectorizer (object): vectorizer object\n",
    "    lda_model (object): lda model object\n",
    "\n",
    "    Returns:\n",
    "    str: dominant aspect of text\n",
    "    \"\"\"\n",
    "    tfidf_vector = vectorizer.transform(df['content'])\n",
    "    aspects = lda_model.transform(tfidf_vector)\n",
    "    dominant_aspect = aspects.argmax(axis=1)\n",
    "    df['topic'] = pd.Series(dominant_aspect).apply(lambda x: list(topic_dict.keys())[x])\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc61427-a0a4-4b34-8881-c42c727f48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, object, dataset_name=None, vectorizer=None, lda_model=None, vader_model=None, topic_dict=None) -> None:\n",
    "        self.name = dataset_name\n",
    "        # main template method\n",
    "        self.data = self.parse(object)\n",
    "        # insert hook: if vectorizer, lda_model and topic_dict are not provided, prepare them\n",
    "        if not vectorizer or not lda_model or not topic_dict or not vader_model:\n",
    "            self.prepare_ABSA()\n",
    "        else:\n",
    "            self.vectorizer = vectorizer\n",
    "            self.lda_model = lda_model\n",
    "            self.topic_dict = topic_dict\n",
    "            self.vader_model = vader_model\n",
    "        self.perform_ABSA()\n",
    "    \n",
    "    # MAIN TEMPLATE METHODS\n",
    "    def prepare_ABSA(self):\n",
    "        \"\"\"\n",
    "        Prepare ABSA by setting up vectorizer and LDA model\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.vectorizer, self.lda_model, self.vader_model and self.topic_dict\n",
    "        \"\"\"\n",
    "        print(\"Preparing dataset for ABSA...\")\n",
    "        self.prepare_vectorizer()\n",
    "        self.prepare_lda_model()\n",
    "        self.prepare_vader_model()\n",
    "        print(\"Dataset prepared for ABSA\")\n",
    "\n",
    "    def perform_ABSA(self):\n",
    "        \"\"\"\n",
    "        Perform ABSA on text data\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.data containing \"content\", \"sentiment\" and \"aspect\" columns\n",
    "        \"\"\"\n",
    "        print(\"Performing ABSA...\")\n",
    "        print(\"Extracting aspects...\")\n",
    "        self.aspects = self.extract_aspect()\n",
    "        print(\"Getting sentiment...\")\n",
    "        self.get_sentiment()\n",
    "        print(\"ABSA completed\")\n",
    "        \n",
    "\n",
    "    # FUNCTIONAL METHODS\n",
    "    def prepare_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Prepare vectorizer for text data\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.X, self.vectorizer and self.feature_names\n",
    "        \"\"\"\n",
    "        print(f\"Preparing vectorizer...\")\n",
    "        # initialize and train vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=50, ngram_range=(1, 2))\n",
    "        self.X = vectorizer.fit_transform(self.data['content'])\n",
    "        self.vectorizer = vectorizer\n",
    "        # retrieve feature names\n",
    "        self.feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # save vectorizer\n",
    "        with open(f'../models/{self.name}_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(vectorizer, f)\n",
    "        print(f\"Vectorizer saved as {self.name}_vectorizer.pkl\")\n",
    "        return\n",
    "\n",
    "    def prepare_lda_model(self):\n",
    "        \"\"\"\n",
    "        Prepare LDA model, extract topics and generate titles using chatgpt\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.lda_model and self.topic_dict\n",
    "        \"\"\"\n",
    "        print(f\"Preparing LDA model...\")\n",
    "        # initialize all dependencies for lda model\n",
    "        topic_dict = collections.defaultdict(list)\n",
    "        openai_model = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "        lda_model.fit(self.X)\n",
    "        self.lda_model = lda_model\n",
    "\n",
    "        # document_topics = lda_model.transform(self.X)\n",
    "        # dominant_topic = document_topics.argmax(axis=1)\n",
    "        \n",
    "        # get the top 50 features for each topic\n",
    "        topics = self.lda_model.components_\n",
    "\n",
    "        for idx, topic in enumerate(topics):\n",
    "            top_features = [self.feature_names[j] for j in topic.argsort()[:-20]]\n",
    "            # feed chatgpt the top 20 features and generate a title\n",
    "            prompt = f\"\"\"Generate a unique noun phrase or one-word topic for posts that contain the following features. \n",
    "            This topic will be used for Aspect-Based Sentiment Analysis on social media data. \n",
    "            Ensure the topic is different from previously generated topics. \n",
    "            Feature names:\\n{\", \".join(top_features)}\\nTopic:\"\"\"\n",
    "            prompt = PromptTemplate.from_template(prompt)\n",
    "            response = openai_model.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.template}],\n",
    "                max_tokens=10,\n",
    "                temperature=1,\n",
    "            )\n",
    "\n",
    "            title = response.choices[0].message.content.strip()\n",
    "            # deal with duplicate titles\n",
    "            if title in topic_dict:\n",
    "                title = title + \"_\" + str(idx)\n",
    "            # add title to topic dictionary\n",
    "            topic_dict[title] = [self.feature_names[i] for i in topic.argsort()]\n",
    "\n",
    "        self.topic_dict = topic_dict\n",
    "\n",
    "        # save lda model and topic dictionary\n",
    "        with open(f'../models/{self.name}_lda_model.pkl', 'wb') as f:\n",
    "            pickle.dump(lda_model, f)\n",
    "        with open(f'../data/{self.name}_topic_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(topic_dict, f)\n",
    "\n",
    "        print(f\"LDA model saved as {self.name}_lda_model.pkl\")\n",
    "        print(f\"Topic dictionary saved as {self.name}_topic_dict.pkl\")\n",
    "        return\n",
    "\n",
    "    def prepare_vader_model(self):\n",
    "        \"\"\"\n",
    "        Prepare VADER model for sentiment analysis\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.vader_model\n",
    "        \"\"\"\n",
    "        print(f\"Preparing VADER model...\")\n",
    "        self.vader_model = SentimentIntensityAnalyzer()\n",
    "        with open(f'../models/{self.name}_vader_model.pkl', 'wb') as f:\n",
    "            pickle.dump(self.vader_model, f)\n",
    "\n",
    "        print(f\"VADER model saved as {self.name}_vader_model.pkl\")\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse(self, json_object: object) -> object:\n",
    "        \"\"\"\n",
    "        Abstract method to parse JSON object to be implemented by child class.\n",
    "\n",
    "        Return:\n",
    "        dataframe containing \"content\" column\n",
    "        \"\"\"\n",
    "\n",
    "    def extract_aspect(self):  #check what does extract aspect do \n",
    "        \"\"\"\n",
    "        Extract aspects from self.data using LDA model\n",
    "\n",
    "        Returns:\n",
    "        list: list of dominant aspects in self.data\n",
    "        \"\"\"\n",
    "        print(\"Extracting aspects\")\n",
    "        # vectorize text\n",
    "        return get_aspect(self.data, self.vectorizer, self.lda_model, self.topic_dict)\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        \"\"\"\n",
    "        Get sentiment of text using VADER\n",
    "\n",
    "        Returns:\n",
    "        float: sentiment score\n",
    "        \"\"\"\n",
    "        self.data['sentiment'] = self.data['content'].apply(lambda x: self.vader_model.polarity_scores(x)['compound'])\n",
    "        return\n",
    "\n",
    "    # def generate_word_clouds(self):\n",
    "    #     \"\"\"\n",
    "    #     Generate word clouds for each topic in the topic dictionary.\n",
    "    #     \"\"\"\n",
    "    #     if not hasattr(self, 'topic_dict') or not self.topic_dict:\n",
    "    #         print(\"Topic dictionary is not defined.\")\n",
    "    #         return\n",
    "        \n",
    "    #     for topic, keywords in self.topic_dict.items():\n",
    "    #         text = ' '.join(keywords)\n",
    "            \n",
    "    #         wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    #         plt.figure(figsize=(10, 5))\n",
    "    #         plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    #         plt.axis(\"off\")\n",
    "    #         plt.title(f\"Word Cloud for Topic: {topic}\")\n",
    "    #         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac7d7672-edc7-4207-a043-67b64b6ad62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_Dataset(Dataset):\n",
    "    def parse(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Method to preprocess text data from a DataFrame.\n",
    "\n",
    "        Args:\n",
    "        df (pd.DataFrame): DataFrame to parse and preprocess.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: Processed DataFrame.\n",
    "        \"\"\"\n",
    "        print(\"Parsing DataFrame\")\n",
    "\n",
    "        if \"title\" in df.columns:\n",
    "            df[\"content\"] = df[\"content\"].apply(translate_text)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"\", np.nan)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "            df[\"title\"] = df[\"title\"].apply(translate_text)\n",
    "            df[\"title\"] = df[\"title\"].replace(\"\", np.nan)\n",
    "            df[\"title\"] = df[\"title\"].replace(\"[deleted]\", np.nan)\n",
    "            \n",
    "            df.dropna(inplace=True)\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            df[\"content\"] = df[\"title\"] + \" \" + df[\"content\"]\n",
    "            df = df.drop(columns=['title', 'username', 'commentCount', 'score', 'subreddit'])\n",
    "\n",
    "        else:\n",
    "            df[\"content\"] = df[\"content\"].apply(translate_text)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"\", np.nan)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "            \n",
    "            df.dropna(inplace=True)\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            df = df.drop(columns=['username', 'score', 'post_id', 'parent_id'])\n",
    "\n",
    "        df[\"content\"] = df[\"content\"].apply(preprocess_text)\n",
    "        \n",
    "        print(f\"Parsed DataFrame with shape: {df.shape}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19a46845-cbe3-41d6-94fa-fc405e2a98f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (400, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as AA_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as AA_lda_model.pkl\n",
      "Topic dictionary saved as AA_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as AA_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (2827, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as DL_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as DL_lda_model.pkl\n",
      "Topic dictionary saved as DL_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as DL_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (10209, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as F9_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as F9_lda_model.pkl\n",
      "Topic dictionary saved as F9_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as F9_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (2800, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as HA_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as HA_lda_model.pkl\n",
      "Topic dictionary saved as HA_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as HA_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (53295, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as WN_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as WN_lda_model.pkl\n",
      "Topic dictionary saved as WN_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as WN_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n"
     ]
    }
   ],
   "source": [
    "codes = {code: df for code, df in comments_df.groupby('Code')}\n",
    "for code, df in codes.items():\n",
    "    data = DF_Dataset(df, dataset_name=code)\n",
    "    data.data.to_csv(f\"{code}_comments.csv\", index=False)\n",
    "    # df1 = pd.DataFrame(data.data).topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373be9a6-f48c-451c-b1b2-09af795a4f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6b2db-1d4e-412d-b608-11e81084e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.generate_word_clouds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe61082-865b-432f-ab81-fc76d9efe032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def invoke_claimbuster_api(input_claim):\n",
    "#     try:\n",
    "#         api_response = requests.get(\n",
    "#             url=f\"https://idir.uta.edu/claimbuster/api/v2/score/text/{input_claim}\", headers={\"x-api-key\": os.environ.get('CLAIMBUSTER_API_KEY')})\n",
    "#         data = api_response.json()\n",
    "#         if data[\"results\"]:\n",
    "#             return data[\"results\"][0][\"score\"]\n",
    "#         return 0\n",
    "#     except Exception as e:  \n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6ba81-38ee-4edd-a710-a201de99b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_df['claimScore'] = posts_df.content.apply(invoke_claimbuster_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e409bd-56ab-40cf-a41b-e816729903e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_df['claimScore'] = comments_df.content.apply(invoke_claimbuster_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decccd6f-08e7-42a4-aa1a-af5576222879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b76af7-5997-4ecb-9d77-32b954ca5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    s3.put_object(\n",
    "        Bucket='is459-project-output-data', \n",
    "        Key=f'reddit/posts/reddit_final_posts_{datetime.utcnow().strftime(\"%Y-%m-%d)}.csv',\n",
    "        Body=json.dumps(posts),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    print(\"Files uploaded to S3 successfully\")\n",
    "except Exception as e:\n",
    "    print(\"Error uploading to S3: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e6c22-3025-4420-bba2-db60fc311170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c479d33-3abf-44d8-a1ec-8bb73bf8e327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
