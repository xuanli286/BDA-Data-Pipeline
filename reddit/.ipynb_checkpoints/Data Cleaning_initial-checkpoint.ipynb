{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042bedf7-5669-4df7-90cb-fc12d4f0eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deep_translator\n",
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7095d00-d0fd-4fcd-b0e0-2a7f25a98522",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e814c-f869-4f66-b8de-e00605ebf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f66f9-9d1b-4700-ba3a-14223e24fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72133977-819e-4128-b948-c5f06b0aab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xuanli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/xuanli/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from abc import abstractmethod\n",
    "import collections\n",
    "from datetime import datetime\n",
    "from deep_translator import GoogleTranslator\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bbb6ee-d1f7-4ce7-bb72-d5e2399b1cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93523855-041b-46fa-be0c-2bc79b12fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_df = pd.read_json('../data/reddit_posts.json')\n",
    "# comments_df = pd.read_json('../data/reddit_comments.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e028d58-f4cd-423b-b7d4-35b37616514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from S3 bucket - temporary \n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def load_json(prefix):\n",
    "    response = s3.list_objects_v2(Bucket=\"is459-project-data\", Prefix=prefix)\n",
    "    json_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.json')]\n",
    "    combined_data = []\n",
    "\n",
    "    for file_key in json_files:\n",
    "        obj = s3.get_object(Bucket=\"is459-project-data\", Key=file_key)\n",
    "        data = json.load(BytesIO(obj['Body'].read()))\n",
    "        if isinstance(data, list):\n",
    "            combined_data.extend(data)\n",
    "        else:\n",
    "            combined_data.append(data)\n",
    "            \n",
    "    df = pd.DataFrame(combined_data)\n",
    "    return df\n",
    "\n",
    "posts_df = load_json(\"reddit/posts/\")\n",
    "comments_df = load_json(\"reddit/comments/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab406f5-cfc4-4d34-b6a9-3986440830ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5209, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ecd653-9bde-4f8d-a215-2076627b0d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122499, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef5ba38-ce19-4fab-8265-8b4a3947e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df.replace(\"\", np.nan)\n",
    "posts_df.dropna(inplace=True)\n",
    "\n",
    "comments_df = comments_df.replace(\"\", np.nan)\n",
    "comments_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c92ff0f-3516-4fd9-b60d-03c0caf4152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = {\n",
    "    'SouthwestAirlines': 'WN', \n",
    "    'Southwest_Airlines': 'WN', \n",
    "    'AmericanAir': 'AA',\n",
    "    'DeltaAirlines': 'DL',\n",
    "    'HawaiianAirlines': 'HA',\n",
    "    'frontierairlines': 'F9',\n",
    "    'delta': 'DL'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acac675c-2a46-4056-84ed-4e9d34c251b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df['Code'] = posts_df['subreddit'].map(airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d36850-0f6f-4eb1-b748-dd3b7bc43dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df.drop_duplicates(subset=\"id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8cc77a-162b-492a-867f-9127c30b75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df.drop_duplicates(subset=\"id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b7d6877-6071-4209-8ab9-f0c2785796a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3496, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8aa442-2b6a-4e5f-899e-d2eef2ea432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118698, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2297862-f719-4c60-ad3c-732d498415a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_post_dict = posts_df.set_index('id')['Code'].to_dict()\n",
    "comments_df['Code'] = comments_df['post_id'].map(code_post_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f89aad8-47af-45d6-a6cc-6a62b96fdb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df.dropna()\n",
    "comments_df = comments_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19648196-2638-487a-b81d-3b40203aa10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xuanli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "DetectorFactory.seed = 42\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45af1fac-5f63-4296-8e2d-15746ecb6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = GoogleTranslator(source='auto', target='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba55e52e-863d-4d6c-b642-b6ba0fbdd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Check if text is in English\n",
    "\n",
    "    Args:\n",
    "    text (str): text to check\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def chunk_text(text, max_length=5000):\n",
    "    chunks = []\n",
    "    while len(text) > max_length:\n",
    "        split_index = text[:max_length].rfind(' ')\n",
    "        if split_index == -1:\n",
    "            split_index = max_length\n",
    "        chunks.append(text[:split_index])\n",
    "        text = text[split_index:].strip()\n",
    "    chunks.append(text)\n",
    "    return chunks\n",
    "\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        if not is_english(text):\n",
    "            if len(text) > 5000:\n",
    "                chunks = chunk_text(text)\n",
    "                translated_chunks = [translator.translate(chunk) for chunk in chunks]\n",
    "                return ' '.join(translated_chunks)\n",
    "            else:\n",
    "                return translator.translate(text)\n",
    "        else:\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf69b6ef-988e-4a7a-97e2-67cc049e8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by converting to lowercase, removing numbers, punctuation, and stopwords\n",
    "\n",
    "    Args:\n",
    "    text (str): text to preprocess\n",
    "\n",
    "    Returns:\n",
    "    text (str): preprocessed text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0db8888-7233-453c-b9f9-dfef92c2dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect(df, vectorizer=None, lda_model=None, topic_dict=None):\n",
    "    \"\"\"\n",
    "    Get aspect of text using LDA model.\n",
    "\n",
    "    Args:\n",
    "    text (str): text to extract aspect from\n",
    "    vectorizer (object): vectorizer object\n",
    "    lda_model (object): lda model object\n",
    "\n",
    "    Returns:\n",
    "    str: dominant aspect of text\n",
    "    \"\"\"\n",
    "    tfidf_vector = vectorizer.transform(df['content'])\n",
    "    aspects = lda_model.transform(tfidf_vector)\n",
    "    dominant_aspect = aspects.argmax(axis=1)\n",
    "    df['topic'] = pd.Series(dominant_aspect).apply(lambda x: list(topic_dict.keys())[x])\n",
    "    df['topic'] = df['topic'].str.replace(f\"[{string.punctuation}\\d]\", \"\", regex=True)\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bc61427-a0a4-4b34-8881-c42c727f48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, object, dataset_name=None, vectorizer=None, lda_model=None, vader_model=None, topic_dict=None) -> None:\n",
    "        self.name = dataset_name\n",
    "        # main template method\n",
    "        self.data = self.parse(object)\n",
    "        # insert hook: if vectorizer, lda_model and topic_dict are not provided, prepare them\n",
    "        if not vectorizer or not lda_model or not topic_dict or not vader_model:\n",
    "            self.prepare_ABSA()\n",
    "        else:\n",
    "            self.vectorizer = vectorizer\n",
    "            self.lda_model = lda_model\n",
    "            self.topic_dict = topic_dict\n",
    "            self.vader_model = vader_model\n",
    "        self.perform_ABSA()\n",
    "    \n",
    "    # MAIN TEMPLATE METHODS\n",
    "    def prepare_ABSA(self):\n",
    "        \"\"\"\n",
    "        Prepare ABSA by setting up vectorizer and LDA model\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.vectorizer, self.lda_model, self.vader_model and self.topic_dict\n",
    "        \"\"\"\n",
    "        print(\"Preparing dataset for ABSA...\")\n",
    "        self.prepare_vectorizer()\n",
    "        self.prepare_lda_model()\n",
    "        self.prepare_vader_model()\n",
    "        print(\"Dataset prepared for ABSA\")\n",
    "\n",
    "    def perform_ABSA(self):\n",
    "        \"\"\"\n",
    "        Perform ABSA on text data\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.data containing \"content\", \"sentiment\" and \"aspect\" columns\n",
    "        \"\"\"\n",
    "        print(\"Performing ABSA...\")\n",
    "        print(\"Extracting aspects...\")\n",
    "        self.aspects = self.extract_aspect()\n",
    "        print(\"Getting sentiment...\")\n",
    "        self.get_sentiment()\n",
    "        print(\"ABSA completed\")\n",
    "        \n",
    "\n",
    "    # FUNCTIONAL METHODS\n",
    "    def prepare_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Prepare vectorizer for text data\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.X, self.vectorizer and self.feature_names\n",
    "        \"\"\"\n",
    "        print(f\"Preparing vectorizer...\")\n",
    "        # initialize and train vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=50, ngram_range=(1, 2))\n",
    "        self.X = vectorizer.fit_transform(self.data['content'])\n",
    "        self.vectorizer = vectorizer\n",
    "        # retrieve feature names\n",
    "        self.feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # save vectorizer\n",
    "        with open(f'../models/{self.name}_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(vectorizer, f)\n",
    "        print(f\"Vectorizer saved as {self.name}_vectorizer.pkl\")\n",
    "        return\n",
    "\n",
    "    def prepare_lda_model(self):\n",
    "        \"\"\"\n",
    "        Prepare LDA model, extract topics and generate titles using chatgpt\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.lda_model and self.topic_dict\n",
    "        \"\"\"\n",
    "        print(f\"Preparing LDA model...\")\n",
    "        # initialize all dependencies for lda model\n",
    "        topic_dict = collections.defaultdict(list)\n",
    "        openai_model = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "        lda_model.fit(self.X)\n",
    "        self.lda_model = lda_model\n",
    "\n",
    "        # document_topics = lda_model.transform(self.X)\n",
    "        # dominant_topic = document_topics.argmax(axis=1)\n",
    "        \n",
    "        # get the top 50 features for each topic\n",
    "        topics = self.lda_model.components_\n",
    "\n",
    "        for idx, topic in enumerate(topics):\n",
    "            top_features = [self.feature_names[j] for j in topic.argsort()[:-20]]\n",
    "            # feed chatgpt the top 20 features and generate a title\n",
    "            prompt = f\"\"\"Generate a unique noun phrase or one-word topic for posts that contain the following features. \n",
    "            This topic will be used for Aspect-Based Sentiment Analysis on social media data. \n",
    "            Ensure the topic is different from previously generated topics. \n",
    "            Feature names:\\n{\", \".join(top_features)}\\nTopic:\"\"\"\n",
    "            prompt = PromptTemplate.from_template(prompt)\n",
    "            response = openai_model.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.template}],\n",
    "                max_tokens=10,\n",
    "                temperature=1,\n",
    "            )\n",
    "\n",
    "            title = response.choices[0].message.content.strip()\n",
    "            # deal with duplicate titles\n",
    "            if title in topic_dict:\n",
    "                title = title + \"_\" + str(idx)\n",
    "            # add title to topic dictionary\n",
    "            topic_dict[title] = [self.feature_names[i] for i in topic.argsort()]\n",
    "\n",
    "        self.topic_dict = topic_dict\n",
    "\n",
    "        # save lda model and topic dictionary\n",
    "        with open(f'../models/{self.name}_lda_model.pkl', 'wb') as f:\n",
    "            pickle.dump(lda_model, f)\n",
    "        with open(f'../data/{self.name}_topic_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(topic_dict, f)\n",
    "\n",
    "        print(f\"LDA model saved as {self.name}_lda_model.pkl\")\n",
    "        print(f\"Topic dictionary saved as {self.name}_topic_dict.pkl\")\n",
    "        return\n",
    "\n",
    "    def prepare_vader_model(self):\n",
    "        \"\"\"\n",
    "        Prepare VADER model for sentiment analysis\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.vader_model\n",
    "        \"\"\"\n",
    "        print(f\"Preparing VADER model...\")\n",
    "        self.vader_model = SentimentIntensityAnalyzer()\n",
    "        with open(f'../models/{self.name}_vader_model.pkl', 'wb') as f:\n",
    "            pickle.dump(self.vader_model, f)\n",
    "\n",
    "        print(f\"VADER model saved as {self.name}_vader_model.pkl\")\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse(self, json_object: object) -> object:\n",
    "        \"\"\"\n",
    "        Abstract method to parse JSON object to be implemented by child class.\n",
    "\n",
    "        Return:\n",
    "        dataframe containing \"content\" column\n",
    "        \"\"\"\n",
    "\n",
    "    def extract_aspect(self):  #check what does extract aspect do \n",
    "        \"\"\"\n",
    "        Extract aspects from self.data using LDA model\n",
    "\n",
    "        Returns:\n",
    "        list: list of dominant aspects in self.data\n",
    "        \"\"\"\n",
    "        print(\"Extracting aspects\")\n",
    "        # vectorize text\n",
    "        return get_aspect(self.data, self.vectorizer, self.lda_model, self.topic_dict)\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        \"\"\"\n",
    "        Get sentiment of text using VADER\n",
    "\n",
    "        Returns:\n",
    "        float: sentiment score\n",
    "        \"\"\"\n",
    "        self.data['sentiment'] = self.data['content'].apply(lambda x: self.vader_model.polarity_scores(x)['compound'])\n",
    "        return\n",
    "\n",
    "    # def generate_word_clouds(self):\n",
    "    #     \"\"\"\n",
    "    #     Generate word clouds for each topic in the topic dictionary.\n",
    "    #     \"\"\"\n",
    "    #     if not hasattr(self, 'topic_dict') or not self.topic_dict:\n",
    "    #         print(\"Topic dictionary is not defined.\")\n",
    "    #         return\n",
    "        \n",
    "    #     for topic, keywords in self.topic_dict.items():\n",
    "    #         text = ' '.join(keywords)\n",
    "            \n",
    "    #         wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    #         plt.figure(figsize=(10, 5))\n",
    "    #         plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    #         plt.axis(\"off\")\n",
    "    #         plt.title(f\"Word Cloud for Topic: {topic}\")\n",
    "    #         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac7d7672-edc7-4207-a043-67b64b6ad62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_Dataset(Dataset):\n",
    "    def parse(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Method to preprocess text data from a DataFrame.\n",
    "\n",
    "        Args:\n",
    "        df (pd.DataFrame): DataFrame to parse and preprocess.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: Processed DataFrame.\n",
    "        \"\"\"\n",
    "        print(\"Parsing DataFrame\")\n",
    "\n",
    "        if \"title\" in df.columns:\n",
    "            df[\"content\"] = df[\"content\"].apply(translate_text)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"\", np.nan)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "            df[\"title\"] = df[\"title\"].apply(translate_text)\n",
    "            df[\"title\"] = df[\"title\"].replace(\"\", np.nan)\n",
    "            df[\"title\"] = df[\"title\"].replace(\"[deleted]\", np.nan)\n",
    "            \n",
    "            df.dropna(inplace=True)\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            df[\"content\"] = df[\"title\"] + \" \" + df[\"content\"]\n",
    "            df = df.drop(columns=['title', 'username', 'commentCount', 'score', 'subreddit'])\n",
    "\n",
    "        else:\n",
    "            df[\"content\"] = df[\"content\"].apply(translate_text)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"\", np.nan)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "            \n",
    "            df.dropna(inplace=True)\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            df = df.drop(columns=['username', 'score', 'post_id', 'parent_id'])\n",
    "\n",
    "        df[\"content\"] = df[\"content\"].apply(preprocess_text)\n",
    "        \n",
    "        print(f\"Parsed DataFrame with shape: {df.shape}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f27c7-e5cb-4256-94c5-428599722d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_df['content'] = posts_df['content'].astype(str)\n",
    "# posts_df[\"content\"] = posts_df[\"content\"].apply(translate_text)\n",
    "# posts_df[\"content\"] = posts_df[\"content\"].replace(\"\", np.nan)\n",
    "# posts_df[\"content\"] = posts_df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "# posts_df[\"title\"] = posts_df[\"title\"].apply(translate_text)\n",
    "# posts_df[\"title\"] = posts_df[\"title\"].replace(\"\", np.nan)\n",
    "# posts_df[\"title\"] = posts_df[\"title\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "# posts_df = posts_df.dropna(subset=[\"content\", \"title\"]).reset_index(drop=True)\n",
    "\n",
    "# posts_df[\"content\"] = posts_df[\"title\"] + \" \" + posts_df[\"content\"]\n",
    "# posts_df = posts_df.drop(columns=['title', 'username', 'commentCount', 'score', 'subreddit'])\n",
    "\n",
    "# posts_df[\"content\"] = posts_df[\"content\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffd641-f956-4836-909c-84b939e7886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_df['content'] = comments_df['content'].astype(str)\n",
    "# comments_df[\"content\"] = comments_df[\"content\"].apply(translate_text)\n",
    "# comments_df[\"content\"] = comments_df[\"content\"].replace(\"\", np.nan)\n",
    "# comments_df[\"content\"] = comments_df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "# comments_df = comments_df.dropna(subset=[\"content\"]).reset_index(drop=True)\n",
    "\n",
    "# comments_df = comments_df.drop(columns=['username', 'score', 'post_id', 'parent_id'])\n",
    "\n",
    "# comments_df[\"content\"] = comments_df[\"content\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb237453-bda4-47b7-a0c0-4ed3764d7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([posts_df, comments_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1256208-51e4-485d-85e0-7d07c6d4a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19a46845-cbe3-41d6-94fa-fc405e2a98f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (186, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as AA_posts_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as AA_posts_lda_model.pkl\n",
      "Topic dictionary saved as AA_posts_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as AA_posts_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (400, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as AA_comments_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as AA_comments_lda_model.pkl\n",
      "Topic dictionary saved as AA_comments_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as AA_comments_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (1387, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as DL_posts_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as DL_posts_lda_model.pkl\n",
      "Topic dictionary saved as DL_posts_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as DL_posts_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (36317, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as DL_comments_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as DL_comments_lda_model.pkl\n",
      "Topic dictionary saved as DL_comments_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as DL_comments_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (414, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as HA_posts_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as HA_posts_lda_model.pkl\n",
      "Topic dictionary saved as HA_posts_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as HA_posts_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (2800, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as HA_comments_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as HA_comments_lda_model.pkl\n",
      "Topic dictionary saved as HA_comments_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as HA_comments_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (1181, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as WN_posts_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as WN_posts_lda_model.pkl\n",
      "Topic dictionary saved as WN_posts_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as WN_posts_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (53295, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as WN_comments_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as WN_comments_lda_model.pkl\n",
      "Topic dictionary saved as WN_comments_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as WN_comments_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (300, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as F9_posts_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as F9_posts_lda_model.pkl\n",
      "Topic dictionary saved as F9_posts_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as F9_posts_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (10209, 4)\n",
      "Preparing dataset for ABSA...\n",
      "Preparing vectorizer...\n",
      "Vectorizer saved as F9_comments_vectorizer.pkl\n",
      "Preparing LDA model...\n",
      "LDA model saved as F9_comments_lda_model.pkl\n",
      "Topic dictionary saved as F9_comments_topic_dict.pkl\n",
      "Preparing VADER model...\n",
      "VADER model saved as F9_comments_vader_model.pkl\n",
      "Dataset prepared for ABSA\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n"
     ]
    }
   ],
   "source": [
    "codes = comments_df['Code'].unique()\n",
    "for code in codes:\n",
    "    posts_data = DF_Dataset(posts_df[posts_df['Code'] == code].copy(), dataset_name=f\"{code}_posts\")\n",
    "    posts_data.data.to_csv(f\"{code}_posts.csv\", index=False)\n",
    "\n",
    "    comments_data = DF_Dataset(comments_df[comments_df['Code'] == code].copy(), dataset_name=f\"{code}_comments\")\n",
    "    comments_data.data.to_csv(f\"{code}_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373be9a6-f48c-451c-b1b2-09af795a4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow.parquet as pq\n",
    "# table1 = pq.read_table('part-00000-bd8a5369-965d-4d67-a1c7-1f30a77cb6d3-c000.snappy.parquet')\n",
    "# df1 = table1.to_pandas()\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23adf0-17f3-4961-9f08-30d00e03437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pq.read_table('part-00000-a3621429-5a6f-47f9-bc59-35e72306ebed-c000.snappy.parquet')\n",
    "df2 = table2.to_pandas()\n",
    "df2 = df2[sorted(df2.columns)]\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f390387-612f-4609-afcd-a720af337c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213dea2a-2864-44af-a954-8ba2aec2e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df = load_json(\"skytrax/reviews/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df1a4b-f584-4af3-8b69-bf1329a53e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed4771-bd69-47e6-bb38-f8068d8c08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15e512-1cc9-44d0-b1ee-1a64a3c4f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df = skytrax_df.drop_duplicates(subset=[\"airline\", \"username\", \"title\", \"publishedDate\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5401163-c6b1-4994-9759-9ff2c73c3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c017c3f-9887-42a3-b4c5-dba272663b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df['airline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f59d2-f2d8-4bed-bf89-f71d7b4e3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_airlines = {\n",
    "    'southwest-airlines': 'WN', \n",
    "    'american-airlines': 'AA',\n",
    "    'delta-air-lines': 'DL',\n",
    "    'hawaiian-airlines': 'HA',\n",
    "    'frontier-airlines': 'F9'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6bfc7-2040-4311-8268-e9a8111e11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df['Code'] = skytrax_df['airline'].map(skytrax_airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9287a-fafd-4f48-a5cf-cd719f2f45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df['publishedDate'] = skytrax_df['publishedDate'].apply(lambda x: re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', x))\n",
    "skytrax_df['publishedDate'] = pd.to_datetime(skytrax_df['publishedDate'], errors='coerce')\n",
    "skytrax_df['publishedDate'] = skytrax_df['publishedDate'].dt.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7dc3ad-472c-4dc2-af9c-acef1e5c8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df[\"title\"] = skytrax_df[\"title\"].apply(translate_text)\n",
    "skytrax_df[\"title\"] = skytrax_df[\"title\"].replace(\"\", np.nan)\n",
    "skytrax_df[\"title\"] = skytrax_df[\"title\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "skytrax_df[\"review\"] = skytrax_df[\"review\"].apply(translate_text)\n",
    "skytrax_df[\"review\"] = skytrax_df[\"review\"].replace(\"\", np.nan)\n",
    "skytrax_df[\"review\"] = skytrax_df[\"review\"].replace(\"[deleted]\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f61bd-c42d-4e10-943d-6f19f80eb5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a6e7b-2ff5-4e7b-9080-526f4610d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df.dropna(inplace=True)\n",
    "skytrax_df = skytrax_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27290951-5641-4042-ac6e-0ab518e565be",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6b2db-1d4e-412d-b608-11e81084e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.generate_word_clouds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe61082-865b-432f-ab81-fc76d9efe032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def invoke_claimbuster_api(input_claim):\n",
    "#     try:\n",
    "#         api_response = requests.get(\n",
    "#             url=f\"https://idir.uta.edu/claimbuster/api/v2/score/text/{input_claim}\", headers={\"x-api-key\": os.environ.get('CLAIMBUSTER_API_KEY')})\n",
    "#         data = api_response.json()\n",
    "#         if data[\"results\"]:\n",
    "#             return data[\"results\"][0][\"score\"]\n",
    "#         return 0\n",
    "#     except Exception as e:  \n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6ba81-38ee-4edd-a710-a201de99b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_df['claimScore'] = posts_df.content.apply(invoke_claimbuster_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e409bd-56ab-40cf-a41b-e816729903e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_df['claimScore'] = comments_df.content.apply(invoke_claimbuster_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decccd6f-08e7-42a4-aa1a-af5576222879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b76af7-5997-4ecb-9d77-32b954ca5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    s3.put_object(\n",
    "        Bucket='is459-project-output-data', \n",
    "        Key=f'reddit/posts/reddit_final_posts_{datetime.utcnow().strftime(\"%Y-%m-%d)}.csv',\n",
    "        Body=json.dumps(posts),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    print(\"Files uploaded to S3 successfully\")\n",
    "except Exception as e:\n",
    "    print(\"Error uploading to S3: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e6c22-3025-4420-bba2-db60fc311170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c479d33-3abf-44d8-a1ec-8bb73bf8e327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
