{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042bedf7-5669-4df7-90cb-fc12d4f0eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deep_translator\n",
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c34ba-a112-45ae-adc4-240cf1907909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7095d00-d0fd-4fcd-b0e0-2a7f25a98522",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e814c-f869-4f66-b8de-e00605ebf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f66f9-9d1b-4700-ba3a-14223e24fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72133977-819e-4128-b948-c5f06b0aab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xuanli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/xuanli/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from abc import abstractmethod\n",
    "import collections\n",
    "from datetime import datetime\n",
    "from deep_translator import GoogleTranslator\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import uuid\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bbb6ee-d1f7-4ce7-bb72-d5e2399b1cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93523855-041b-46fa-be0c-2bc79b12fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_df = pd.read_json('../data/reddit_posts.json')\n",
    "# comments_df = pd.read_json('../data/reddit_comments.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e028d58-f4cd-423b-b7d4-35b37616514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NEW data from S3 bucket - temporary \n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def load_json(prefix):\n",
    "    response = s3.list_objects_v2(Bucket=\"is459-project-data\", Prefix=prefix)\n",
    "    json_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.json')]\n",
    "    combined_data = []\n",
    "\n",
    "    for file_key in json_files:\n",
    "        obj = s3.get_object(Bucket=\"is459-project-data\", Key=file_key)\n",
    "        data = json.load(BytesIO(obj['Body'].read()))\n",
    "        if isinstance(data, list):\n",
    "            combined_data.extend(data)\n",
    "        else:\n",
    "            combined_data.append(data)\n",
    "            \n",
    "    df = pd.DataFrame(combined_data)\n",
    "    return df\n",
    "\n",
    "posts_df = load_json(\"reddit/new_posts/\")\n",
    "comments_df = load_json(\"reddit/new_comments/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab406f5-cfc4-4d34-b6a9-3986440830ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ecd653-9bde-4f8d-a215-2076627b0d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2511, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef5ba38-ce19-4fab-8265-8b4a3947e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df.replace(\"\", np.nan)\n",
    "posts_df.dropna(inplace=True)\n",
    "\n",
    "comments_df = comments_df.replace(\"\", np.nan)\n",
    "comments_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c92ff0f-3516-4fd9-b60d-03c0caf4152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = {\n",
    "    'SouthwestAirlines': 'WN', \n",
    "    'Southwest_Airlines': 'WN', \n",
    "    'AmericanAir': 'AA',\n",
    "    'DeltaAirlines': 'DL',\n",
    "    'HawaiianAirlines': 'HA',\n",
    "    'frontierairlines': 'F9',\n",
    "    'delta': 'DL'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acac675c-2a46-4056-84ed-4e9d34c251b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df['Code'] = posts_df['subreddit'].map(airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e25dae-7b84-445e-b17c-ca41ab786733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>username</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1gcmf4z</td>\n",
       "      <td>2024-10-26 14:34:29</td>\n",
       "      <td>new app no bueno</td>\n",
       "      <td>tried to check in right on time and got a weir...</td>\n",
       "      <td>tulipox</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SouthwestAirlines</td>\n",
       "      <td>WN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1gcl4tz</td>\n",
       "      <td>2024-10-26 13:30:42</td>\n",
       "      <td>Just a reminder, Southwest has one of the best...</td>\n",
       "      <td>&gt;Customers who encroach upon any part of the n...</td>\n",
       "      <td>pbjclimbing</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>SouthwestAirlines</td>\n",
       "      <td>WN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1gcebnk</td>\n",
       "      <td>2024-10-26 05:47:41</td>\n",
       "      <td>Portland gate view</td>\n",
       "      <td>Pulling in to the gate today at sunset, DEN to...</td>\n",
       "      <td>Own_Layer_5413</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "      <td>SouthwestAirlines</td>\n",
       "      <td>WN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1gccyqf</td>\n",
       "      <td>2024-10-26 04:18:57</td>\n",
       "      <td>Would I be able to take a parasol on a plane a...</td>\n",
       "      <td>I have a parasol that is 74cm (around 2 and a ...</td>\n",
       "      <td>Aku_Akane</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>SouthwestAirlines</td>\n",
       "      <td>WN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1gcct71</td>\n",
       "      <td>2024-10-26 04:09:26</td>\n",
       "      <td>I have over 100k southwest points. Where would...</td>\n",
       "      <td>Haven’t really flown anywhere since Covid and ...</td>\n",
       "      <td>Ok-Lynx-8387</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>SouthwestAirlines</td>\n",
       "      <td>WN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1g8goas</td>\n",
       "      <td>2024-10-21 03:34:31</td>\n",
       "      <td>Flight went great ATL-PA</td>\n",
       "      <td>Don’t know if anyone remembers but I was flyin...</td>\n",
       "      <td>Sonotanabelle</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1g7s9bi</td>\n",
       "      <td>2024-10-20 06:08:17</td>\n",
       "      <td>Refund for exit row seat</td>\n",
       "      <td>Hello! I had a flight a couple of days ago tha...</td>\n",
       "      <td>Exact_Ad1271</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1g7m2u6</td>\n",
       "      <td>2024-10-19 23:53:06</td>\n",
       "      <td>Frontier is the worst.</td>\n",
       "      <td>This is a recent trip to DFW (return trip) \\n\\...</td>\n",
       "      <td>Ok_Locksmith_824</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1g7ei8r</td>\n",
       "      <td>2024-10-19 17:51:04</td>\n",
       "      <td>Go wild! Pricing is down</td>\n",
       "      <td>Go wild! Pricing is down for tomorrow’s bookin...</td>\n",
       "      <td>LNGU1203</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1g7c52t</td>\n",
       "      <td>2024-10-19 16:03:39</td>\n",
       "      <td>Unable to change password</td>\n",
       "      <td>I'm wondering if anyone else has run into this...</td>\n",
       "      <td>Exit-7A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>frontierairlines</td>\n",
       "      <td>F9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                 date  \\\n",
       "0    1gcmf4z  2024-10-26 14:34:29   \n",
       "1    1gcl4tz  2024-10-26 13:30:42   \n",
       "3    1gcebnk  2024-10-26 05:47:41   \n",
       "4    1gccyqf  2024-10-26 04:18:57   \n",
       "5    1gcct71  2024-10-26 04:09:26   \n",
       "..       ...                  ...   \n",
       "152  1g8goas  2024-10-21 03:34:31   \n",
       "153  1g7s9bi  2024-10-20 06:08:17   \n",
       "154  1g7m2u6  2024-10-19 23:53:06   \n",
       "155  1g7ei8r  2024-10-19 17:51:04   \n",
       "156  1g7c52t  2024-10-19 16:03:39   \n",
       "\n",
       "                                                 title  \\\n",
       "0                                     new app no bueno   \n",
       "1    Just a reminder, Southwest has one of the best...   \n",
       "3                                   Portland gate view   \n",
       "4    Would I be able to take a parasol on a plane a...   \n",
       "5    I have over 100k southwest points. Where would...   \n",
       "..                                                 ...   \n",
       "152                           Flight went great ATL-PA   \n",
       "153                           Refund for exit row seat   \n",
       "154                            Frontier is the worst.    \n",
       "155                           Go wild! Pricing is down   \n",
       "156                          Unable to change password   \n",
       "\n",
       "                                               content          username  \\\n",
       "0    tried to check in right on time and got a weir...           tulipox   \n",
       "1    >Customers who encroach upon any part of the n...       pbjclimbing   \n",
       "3    Pulling in to the gate today at sunset, DEN to...    Own_Layer_5413   \n",
       "4    I have a parasol that is 74cm (around 2 and a ...         Aku_Akane   \n",
       "5    Haven’t really flown anywhere since Covid and ...      Ok-Lynx-8387   \n",
       "..                                                 ...               ...   \n",
       "152  Don’t know if anyone remembers but I was flyin...     Sonotanabelle   \n",
       "153  Hello! I had a flight a couple of days ago tha...      Exact_Ad1271   \n",
       "154  This is a recent trip to DFW (return trip) \\n\\...  Ok_Locksmith_824   \n",
       "155  Go wild! Pricing is down for tomorrow’s bookin...          LNGU1203   \n",
       "156  I'm wondering if anyone else has run into this...           Exit-7A   \n",
       "\n",
       "     commentCount  score          subreddit Code  \n",
       "0               2      2  SouthwestAirlines   WN  \n",
       "1              23      8  SouthwestAirlines   WN  \n",
       "3               2     73  SouthwestAirlines   WN  \n",
       "4               4      2  SouthwestAirlines   WN  \n",
       "5              29      1  SouthwestAirlines   WN  \n",
       "..            ...    ...                ...  ...  \n",
       "152             6      6   frontierairlines   F9  \n",
       "153             6      4   frontierairlines   F9  \n",
       "154            43     30   frontierairlines   F9  \n",
       "155             2      2   frontierairlines   F9  \n",
       "156             2      1   frontierairlines   F9  \n",
       "\n",
       "[143 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d36850-0f6f-4eb1-b748-dd3b7bc43dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df.drop_duplicates(subset=\"id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8cc77a-162b-492a-867f-9127c30b75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df.drop_duplicates(subset=\"id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b7d6877-6071-4209-8ab9-f0c2785796a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8aa442-2b6a-4e5f-899e-d2eef2ea432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2511, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2297862-f719-4c60-ad3c-732d498415a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_post_dict = posts_df.set_index('id')['Code'].to_dict()\n",
    "comments_df['Code'] = comments_df['post_id'].map(code_post_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104462ad-6b0b-4993-9afc-060bc08ac648",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df = load_json(\"skytrax/reviews/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fda128ba-377b-46d9-8294-6ab1d8d11354",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df = skytrax_df.drop_duplicates(subset=[\"airline\", \"username\", \"title\", \"publishedDate\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af27dbdf-be24-4615-8e71-aa3ea93b20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_airlines = {\n",
    "    'southwest-airlines': 'WN', \n",
    "    'american-airlines': 'AA',\n",
    "    'delta-air-lines': 'DL',\n",
    "    'hawaiian-airlines': 'HA',\n",
    "    'frontier-airlines': 'F9'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92285bed-c5a8-4f0a-a33e-f8539bf9adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "skytrax_df['Code'] = skytrax_df['airline'].map(skytrax_airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c2f4bf0-def2-44c8-b354-b57598caeed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15305, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skytrax_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05cbdf2b-4ec2-4d0e-9c6d-2054a8c7dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df.dropna()\n",
    "comments_df = comments_df.dropna()\n",
    "skytrax_df = skytrax_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19648196-2638-487a-b81d-3b40203aa10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xuanli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "DetectorFactory.seed = 42\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45af1fac-5f63-4296-8e2d-15746ecb6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = GoogleTranslator(source='auto', target='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba55e52e-863d-4d6c-b642-b6ba0fbdd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Check if text is in English\n",
    "\n",
    "    Args:\n",
    "    text (str): text to check\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def chunk_text(text, max_length=5000):\n",
    "    chunks = []\n",
    "    while len(text) > max_length:\n",
    "        split_index = text[:max_length].rfind(' ')\n",
    "        if split_index == -1:\n",
    "            split_index = max_length\n",
    "        chunks.append(text[:split_index])\n",
    "        text = text[split_index:].strip()\n",
    "    chunks.append(text)\n",
    "    return chunks\n",
    "\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        if not is_english(text):\n",
    "            if len(text) > 5000:\n",
    "                chunks = chunk_text(text)\n",
    "                translated_chunks = [translator.translate(chunk) for chunk in chunks]\n",
    "                return ' '.join(translated_chunks)\n",
    "            else:\n",
    "                return translator.translate(text)\n",
    "        else:\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf69b6ef-988e-4a7a-97e2-67cc049e8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by converting to lowercase, removing numbers, punctuation, and stopwords\n",
    "\n",
    "    Args:\n",
    "    text (str): text to preprocess\n",
    "\n",
    "    Returns:\n",
    "    text (str): preprocessed text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0db8888-7233-453c-b9f9-dfef92c2dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect(df, vectorizer=None, lda_model=None, topic_dict=None):\n",
    "    \"\"\"\n",
    "    Get aspect of text using LDA model.\n",
    "\n",
    "    Args:\n",
    "    text (str): text to extract aspect from\n",
    "    vectorizer (object): vectorizer object\n",
    "    lda_model (object): lda model object\n",
    "\n",
    "    Returns:\n",
    "    str: dominant aspect of text\n",
    "    \"\"\"\n",
    "    tfidf_vector = vectorizer.transform(df['content'])\n",
    "    aspects = lda_model.transform(tfidf_vector)\n",
    "    dominant_aspect = aspects.argmax(axis=1)\n",
    "    df['topic'] = pd.Series(dominant_aspect).apply(lambda x: list(topic_dict.keys())[x])\n",
    "    df['topic'] = df['topic'].str.replace(f\"[{string.punctuation}\\d]\", \"\", regex=True)\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bc61427-a0a4-4b34-8881-c42c727f48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, object, dataset_name=None, vectorizer=None, lda_model=None, vader_model=None, topic_dict=None) -> None:\n",
    "        self.name = dataset_name\n",
    "        # main template method\n",
    "        self.data = self.parse(object)\n",
    "        # insert hook: if vectorizer, lda_model and topic_dict are not provided, prepare them\n",
    "        if not vectorizer or not lda_model or not topic_dict or not vader_model:\n",
    "            self.prepare_ABSA()\n",
    "        else:\n",
    "            self.vectorizer = vectorizer\n",
    "            self.lda_model = lda_model\n",
    "            self.topic_dict = topic_dict\n",
    "            self.vader_model = vader_model\n",
    "        self.perform_ABSA()\n",
    "    \n",
    "    # MAIN TEMPLATE METHODS\n",
    "    def prepare_ABSA(self):\n",
    "        \"\"\"\n",
    "        Prepare ABSA by setting up vectorizer and LDA model\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.vectorizer, self.lda_model, self.vader_model and self.topic_dict\n",
    "        \"\"\"\n",
    "        print(\"Preparing dataset for ABSA...\")\n",
    "        self.prepare_vectorizer()\n",
    "        self.prepare_lda_model()\n",
    "        self.prepare_vader_model()\n",
    "        print(\"Dataset prepared for ABSA\")\n",
    "\n",
    "    def perform_ABSA(self):\n",
    "        \"\"\"\n",
    "        Perform ABSA on text data\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.data containing \"content\", \"sentiment\" and \"aspect\" columns\n",
    "        \"\"\"\n",
    "        print(\"Performing ABSA...\")\n",
    "        print(\"Extracting aspects...\")\n",
    "        self.aspects = self.extract_aspect()\n",
    "        print(\"Getting sentiment...\")\n",
    "        self.get_sentiment()\n",
    "        print(\"ABSA completed\")\n",
    "        \n",
    "\n",
    "    # FUNCTIONAL METHODS\n",
    "    def prepare_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Prepare vectorizer for text data\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.X, self.vectorizer and self.feature_names\n",
    "        \"\"\"\n",
    "        print(f\"Preparing vectorizer...\")\n",
    "        # initialize and train vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=50, ngram_range=(1, 2))\n",
    "        self.X = vectorizer.fit_transform(self.data['content'])\n",
    "        self.vectorizer = vectorizer\n",
    "        # retrieve feature names\n",
    "        self.feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # save vectorizer\n",
    "        with open(f'../models/{self.name}_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(vectorizer, f)\n",
    "        print(f\"Vectorizer saved as {self.name}_vectorizer.pkl\")\n",
    "        return\n",
    "\n",
    "    def prepare_lda_model(self):\n",
    "        \"\"\"\n",
    "        Prepare LDA model, extract topics and generate titles using chatgpt\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.lda_model and self.topic_dict\n",
    "        \"\"\"\n",
    "        print(f\"Preparing LDA model...\")\n",
    "        # initialize all dependencies for lda model\n",
    "        topic_dict = collections.defaultdict(list)\n",
    "        openai_model = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "        lda_model.fit(self.X)\n",
    "        self.lda_model = lda_model\n",
    "\n",
    "        # document_topics = lda_model.transform(self.X)\n",
    "        # dominant_topic = document_topics.argmax(axis=1)\n",
    "        \n",
    "        # get the top 50 features for each topic\n",
    "        topics = self.lda_model.components_\n",
    "\n",
    "        for idx, topic in enumerate(topics):\n",
    "            top_features = [self.feature_names[j] for j in topic.argsort()[:-20]]\n",
    "            # feed chatgpt the top 20 features and generate a title\n",
    "            prompt = f\"\"\"Generate a unique noun phrase or one-word topic for posts that contain the following features. \n",
    "            This topic will be used for Aspect-Based Sentiment Analysis on social media data. \n",
    "            Ensure the topic is different from previously generated topics. \n",
    "            Feature names:\\n{\", \".join(top_features)}\\nTopic:\"\"\"\n",
    "            prompt = PromptTemplate.from_template(prompt)\n",
    "            response = openai_model.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.template}],\n",
    "                max_tokens=10,\n",
    "                temperature=1,\n",
    "            )\n",
    "\n",
    "            title = response.choices[0].message.content.strip()\n",
    "            # deal with duplicate titles\n",
    "            if title in topic_dict:\n",
    "                title = title + \"_\" + str(idx)\n",
    "            # add title to topic dictionary\n",
    "            topic_dict[title] = [self.feature_names[i] for i in topic.argsort()]\n",
    "\n",
    "        self.topic_dict = topic_dict\n",
    "\n",
    "        # save lda model and topic dictionary\n",
    "        with open(f'../models/{self.name}_lda_model.pkl', 'wb') as f:\n",
    "            pickle.dump(lda_model, f)\n",
    "        with open(f'../data/{self.name}_topic_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(topic_dict, f)\n",
    "\n",
    "        print(f\"LDA model saved as {self.name}_lda_model.pkl\")\n",
    "        print(f\"Topic dictionary saved as {self.name}_topic_dict.pkl\")\n",
    "        return\n",
    "\n",
    "    def prepare_vader_model(self):\n",
    "        \"\"\"\n",
    "        Prepare VADER model for sentiment analysis\n",
    "\n",
    "        Returns:\n",
    "        Modifies self.vader_model\n",
    "        \"\"\"\n",
    "        print(f\"Preparing VADER model...\")\n",
    "        self.vader_model = SentimentIntensityAnalyzer()\n",
    "        with open(f'../models/{self.name}_vader_model.pkl', 'wb') as f:\n",
    "            pickle.dump(self.vader_model, f)\n",
    "\n",
    "        print(f\"VADER model saved as {self.name}_vader_model.pkl\")\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse(self, json_object: object) -> object:\n",
    "        \"\"\"\n",
    "        Abstract method to parse JSON object to be implemented by child class.\n",
    "\n",
    "        Return:\n",
    "        dataframe containing \"content\" column\n",
    "        \"\"\"\n",
    "\n",
    "    def extract_aspect(self):  #check what does extract aspect do \n",
    "        \"\"\"\n",
    "        Extract aspects from self.data using LDA model\n",
    "\n",
    "        Returns:\n",
    "        list: list of dominant aspects in self.data\n",
    "        \"\"\"\n",
    "        print(\"Extracting aspects\")\n",
    "        # vectorize text\n",
    "        return get_aspect(self.data, self.vectorizer, self.lda_model, self.topic_dict)\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        \"\"\"\n",
    "        Get sentiment of text using VADER\n",
    "\n",
    "        Returns:\n",
    "        float: sentiment score\n",
    "        \"\"\"\n",
    "        self.data['sentiment'] = self.data['content'].apply(lambda x: self.vader_model.polarity_scores(x)['compound'])\n",
    "        return\n",
    "\n",
    "    # def generate_word_clouds(self):\n",
    "    #     \"\"\"\n",
    "    #     Generate word clouds for each topic in the topic dictionary.\n",
    "    #     \"\"\"\n",
    "    #     if not hasattr(self, 'topic_dict') or not self.topic_dict:\n",
    "    #         print(\"Topic dictionary is not defined.\")\n",
    "    #         return\n",
    "        \n",
    "    #     for topic, keywords in self.topic_dict.items():\n",
    "    #         text = ' '.join(keywords)\n",
    "            \n",
    "    #         wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    #         plt.figure(figsize=(10, 5))\n",
    "    #         plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    #         plt.axis(\"off\")\n",
    "    #         plt.title(f\"Word Cloud for Topic: {topic}\")\n",
    "    #         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c1d0f9a-0bb0-45c9-b6cd-a63552fd369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_Dataset(Dataset):\n",
    "    def parse(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Method to preprocess text data from a DataFrame.\n",
    "\n",
    "        Args:\n",
    "        df (pd.DataFrame): DataFrame to parse and preprocess.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: Processed DataFrame.\n",
    "        \"\"\"\n",
    "        print(\"Parsing DataFrame\")\n",
    "\n",
    "        if \"publishedDate\" in df.columns:\n",
    "            df[\"review\"] = df[\"review\"].apply(translate_text)\n",
    "            df[\"review\"] = df[\"review\"].replace(\"\", np.nan)\n",
    "            df[\"review\"] = df[\"review\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "            df[\"title\"] = df[\"title\"].apply(translate_text)\n",
    "            df[\"title\"] = df[\"title\"].replace(\"\", np.nan)\n",
    "            df[\"title\"] = df[\"title\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "            df = df.dropna(subset=[\"review\", \"title\"]).reset_index(drop=True)\n",
    "        \n",
    "            df['publishedDate'] = df['publishedDate'].apply(lambda x: re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', x))\n",
    "            df['publishedDate'] = pd.to_datetime(df['publishedDate'], errors='coerce')\n",
    "            df = df.dropna(subset=['publishedDate'])\n",
    "            df['publishedDate'] = df['publishedDate'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            df['content'] = df[\"title\"] + \" \" + df[\"review\"]\n",
    "            df['id'] = [uuid.uuid4() for _ in range(len(df))]\n",
    "            df = df.rename(columns={'publishedDate': 'date'})\n",
    "            df = df.drop(columns=['airline', 'username', 'rating', 'title', 'verified', 'review', 'recommend'])\n",
    "\n",
    "        elif \"title\" in df.columns:\n",
    "            df[\"content\"] = df[\"content\"].apply(translate_text)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"\", np.nan)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "            df[\"title\"] = df[\"title\"].apply(translate_text)\n",
    "            df[\"title\"] = df[\"title\"].replace(\"\", np.nan)\n",
    "            df[\"title\"] = df[\"title\"].replace(\"[deleted]\", np.nan)\n",
    "            \n",
    "            df.dropna(inplace=True)\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            df[\"content\"] = df[\"title\"] + \" \" + df[\"content\"]\n",
    "            df = df.drop(columns=['title', 'username', 'commentCount', 'score', 'subreddit'])\n",
    "\n",
    "        else:\n",
    "            df[\"content\"] = df[\"content\"].apply(translate_text)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"\", np.nan)\n",
    "            df[\"content\"] = df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "            \n",
    "            df.dropna(inplace=True)\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            df = df.drop(columns=['username', 'score', 'post_id', 'parent_id'])\n",
    "            \n",
    "\n",
    "        df[\"content\"] = df[\"content\"].apply(preprocess_text)\n",
    "        df = df[['id', 'date', 'content', 'Code']]\n",
    "        \n",
    "        print(f\"Parsed DataFrame with shape: {df.shape}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce5dce-8ad9-43f6-93a8-978453cf5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_df.loc[:, \"content\"] = posts_df[\"content\"].apply(translate_text)\n",
    "# posts_df.loc[:, \"content\"] = posts_df[\"content\"].replace(\"\", np.nan)\n",
    "# posts_df.loc[:, \"content\"] = posts_df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "# posts_df.loc[:, \"title\"] = posts_df[\"title\"].apply(translate_text)\n",
    "# posts_df.loc[:, \"title\"] = posts_df[\"title\"].replace(\"\", np.nan)\n",
    "# posts_df.loc[:, \"title\"] = posts_df[\"title\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "# posts_df = posts_df.dropna(subset=[\"content\", \"title\"]).reset_index(drop=True)\n",
    "\n",
    "# posts_df.loc[:, \"content\"] = posts_df[\"title\"] + \" \" + posts_df[\"content\"]\n",
    "# posts_df = posts_df.drop(columns=['title', 'username', 'commentCount', 'score', 'subreddit'])\n",
    "\n",
    "# posts_df.loc[:, \"content\"] = posts_df[\"content\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fc126-ecf8-4c6b-88e9-96858fcb7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_df.loc[:, \"content\"] = comments_df[\"content\"].apply(translate_text)\n",
    "# comments_df.loc[:, \"content\"] = comments_df[\"content\"].replace(\"\", np.nan)\n",
    "# comments_df.loc[:, \"content\"] = comments_df[\"content\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "# comments_df = comments_df.dropna(subset=[\"content\"]).reset_index(drop=True)\n",
    "\n",
    "# comments_df = comments_df.drop(columns=['username', 'score', 'post_id', 'parent_id'])\n",
    "\n",
    "# comments_df.loc[:, \"content\"] = comments_df[\"content\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9d6fa-321c-4556-9b3d-a27efb4a06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skytrax_df.loc[:, \"review\"] = skytrax_df[\"review\"].apply(translate_text)\n",
    "# skytrax_df.loc[:, \"review\"] = skytrax_df[\"review\"].replace(\"\", np.nan)\n",
    "# skytrax_df.loc[:, \"review\"] = skytrax_df[\"review\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "# skytrax_df.loc[:, \"title\"] = skytrax_df[\"title\"].apply(translate_text)\n",
    "# skytrax_df.loc[:, \"title\"] = skytrax_df[\"title\"].replace(\"\", np.nan)\n",
    "# skytrax_df.loc[:, \"title\"] = skytrax_df[\"title\"].replace(\"[deleted]\", np.nan)\n",
    "\n",
    "# skytrax_df = skytrax_df.dropna(subset=[\"review\", \"title\"]).reset_index(drop=True)\n",
    "\n",
    "# skytrax_df['publishedDate'] = skytrax_df['publishedDate'].apply(lambda x: re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', x))\n",
    "# skytrax_df['publishedDate'] = pd.to_datetime(skytrax_df['publishedDate'], errors='coerce')\n",
    "# skytrax_df = skytrax_df.dropna(subset=['publishedDate'])\n",
    "# skytrax_df['publishedDate'] = skytrax_df['publishedDate'].dt.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e069b04-4927-4f13-92cc-cc004deeb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skytrax_df.loc[:, 'content'] = skytrax_df[\"title\"] + \" \" + skytrax_df[\"review\"]\n",
    "# skytrax_df = skytrax_df.drop(columns=['airline', 'username', 'rating', 'title', 'verified', 'review', 'recommend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd9232-0174-40c3-a9aa-4eb74553c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import uuid\n",
    "\n",
    "# skytrax_df['id'] = [uuid.uuid4() for _ in range(len(skytrax_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a807bac-cf67-452f-8fc2-9fe357bf3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skytrax_df = skytrax_df.rename(columns={'publishedDate': 'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b3d8a-dd58-42d0-bdfb-34bd057b5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([posts_df, comments_df, skytrax_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bedc51-ac95-46ce-9be4-e89318f157af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb705125-16ce-46e8-8321-7ece0293c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "csv_buffer = io.StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07f679b3-56b2-40e0-9634-4753a54f61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (1751, 4)\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (92, 4)\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n",
      "Parsing DataFrame\n",
      "Parsed DataFrame with shape: (1855, 4)\n",
      "Performing ABSA...\n",
      "Extracting aspects...\n",
      "Extracting aspects\n",
      "Getting sentiment...\n",
      "ABSA completed\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "codes = comments_df['Code'].unique()\n",
    "for code in codes:\n",
    "    try:\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "        # Comments\n",
    "        comments_lda_file = s3.get_object(Bucket=\"is459-project-data\", Key=f\"reddit/models/{code}_comments_lda_model.pkl\")['Body'].read()\n",
    "        comments_lda_model = pickle.loads(comments_lda_file)\n",
    "\n",
    "        comments_vectorizer_file = s3.get_object(Bucket=\"is459-project-data\", Key=f\"reddit/models/{code}_comments_vectorizer.pkl\")['Body'].read()\n",
    "        comments_vectorizer_model = pickle.loads(comments_vectorizer_file)\n",
    "        \n",
    "        comments_topic_dict_file = s3.get_object(Bucket=\"is459-project-data\", Key=f\"reddit/models/{code}_comments_topic_dict.pkl\")['Body'].read()\n",
    "        comments_topic_dict = pickle.loads(comments_topic_dict_file)\n",
    "        \n",
    "        comments_data = DF_Dataset(comments_df[comments_df['Code'] == code].copy(), vectorizer=comments_vectorizer_model, lda_model=comments_lda_model, topic_dict=comments_topic_dict, vader_model=vader)\n",
    "        comments_data.data.to_csv(f\"{code}_comments_{datetime.now().strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "        \n",
    "        # Posts\n",
    "        posts_lda_file = s3.get_object(Bucket=\"is459-project-data\", Key=f\"reddit/models/{code}_posts_lda_model.pkl\")['Body'].read()\n",
    "        posts_lda_model = pickle.loads(posts_lda_file)\n",
    "\n",
    "        posts_vectorizer_file = s3.get_object(Bucket=\"is459-project-data\", Key=f\"reddit/models/{code}_posts_vectorizer.pkl\")['Body'].read()\n",
    "        posts_vectorizer_model = pickle.loads(posts_vectorizer_file)\n",
    "        \n",
    "        posts_topic_dict_file = s3.get_object(Bucket=\"is459-project-data\", Key=f\"reddit/models/{code}_posts_topic_dict.pkl\")['Body'].read()\n",
    "        posts_topic_dict = pickle.loads(posts_topic_dict_file)\n",
    "        \n",
    "        posts_data = DF_Dataset(posts_df[posts_df['Code'] == code].copy(), vectorizer=posts_vectorizer_model, lda_model=posts_lda_model, topic_dict=posts_topic_dict, vader_model=vader)\n",
    "        posts_data.data.to_csv(f\"{code}_posts_{datetime.now().strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "\n",
    "        # Skytrax\n",
    "        skytrax_data = DF_Dataset(skytrax_df[skytrax_df['Code'] == code].copy(), vectorizer=posts_vectorizer_model, lda_model=posts_lda_model, topic_dict=posts_topic_dict, vader_model=vader)\n",
    "        skytrax_data.data.to_csv(f\"{code}_skytrax_{datetime.now().strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "\n",
    "        # posts_data.data.to_csv(csv_buffer, index=False)\n",
    "        # s3.put_object(Bucket=\"is459-project-output-data\", Key=f\"reddit/{code}_posts_{datetime.utcnow().strftime('%Y-%m-%d')}.csv\", Body=csv_buffer.getvalue())\n",
    "\n",
    "        # skytrax_data = DF_Dataset(skytrax_df[skytrax_df['Code'] == code], vectorizer=posts_vectorizer_model, lda_model=posts_lda_model, topic_dict=posts_topic_dict, vader_model=vader)\n",
    "        # skytrax_data.data.to_csv(f\"{code}_skytrax_{datetime.now().strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file from S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081352f9-ccac-476a-9f95-3cc230638265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cdb74b-605f-4140-97ac-6115105fb507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6b2db-1d4e-412d-b608-11e81084e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.generate_word_clouds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe61082-865b-432f-ab81-fc76d9efe032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def invoke_claimbuster_api(input_claim):\n",
    "#     try:\n",
    "#         api_response = requests.get(\n",
    "#             url=f\"https://idir.uta.edu/claimbuster/api/v2/score/text/{input_claim}\", headers={\"x-api-key\": os.environ.get('CLAIMBUSTER_API_KEY')})\n",
    "#         data = api_response.json()\n",
    "#         if data[\"results\"]:\n",
    "#             return data[\"results\"][0][\"score\"]\n",
    "#         return 0\n",
    "#     except Exception as e:  \n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6ba81-38ee-4edd-a710-a201de99b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts_df['claimScore'] = posts_df.content.apply(invoke_claimbuster_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e409bd-56ab-40cf-a41b-e816729903e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_df['claimScore'] = comments_df.content.apply(invoke_claimbuster_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decccd6f-08e7-42a4-aa1a-af5576222879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b76af7-5997-4ecb-9d77-32b954ca5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    s3.put_object(\n",
    "        Bucket='is459-project-output-data', \n",
    "        Key=f'reddit/posts/reddit_final_posts_{datetime.utcnow().strftime(\"%Y-%m-%d)}.csv',\n",
    "        Body=json.dumps(posts),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    print(\"Files uploaded to S3 successfully\")\n",
    "except Exception as e:\n",
    "    print(\"Error uploading to S3: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e6c22-3025-4420-bba2-db60fc311170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c479d33-3abf-44d8-a1ec-8bb73bf8e327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
